# Self-Play Meta-Generation System

## Overview

A sophisticated multi-layered prompt generation system that creates synthetic training data through **meta-prompts** → **system prompts** → **user prompts** → **assistant responses**.

## Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│  SELF-PLAY META-GENERATION PIPELINE                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                   │
│  Layer 1: META PROMPT (Optional - or use templates)             │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ Meta instruction tells model how to generate a system     │  │
│  │ prompt for a specific scenario                            │  │
│  │                                                             │  │
│  │ Input: Tool selection or behavior type                    │  │
│  │ Output: System prompt                                     │  │
│  │                                                             │  │
│  │ Example:                                                   │  │
│  │ "Generate a system prompt that instructs a user to        │  │
│  │  create a request using vaultManager_createFolder.        │  │
│  │  The system prompt should tell them they have an          │  │
│  │  Obsidian vault and need to organize their notes..."      │  │
│  └───────────────────────────────────────────────────────────┘  │
│                              ↓                                    │
│  Layer 2: SYSTEM PROMPT (Generated or template)                 │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ Tool-based (70%):                                          │  │
│  │ "You are a user with an Obsidian vault who needs to       │  │
│  │  complete a task using the vaultManager_createFolder      │  │
│  │  tool. Generate a realistic request that includes what    │  │
│  │  you want to do, any specific folders mentioned, and      │  │
│  │  context about why you need it."                          │  │
│  │                                                             │  │
│  │ Behavioral (30%):                                          │  │
│  │ "<session_context>                                        │  │
│  │  IMPORTANT: When using tools, include sessionId and       │  │
│  │  workspaceId...                                           │  │
│  │  </session_context>                                       │  │
│  │  <available_workspaces>...</available_workspaces>"        │  │
│  └───────────────────────────────────────────────────────────┘  │
│                              ↓                                    │
│  Layer 3: USER PROMPT (Generated by model using system prompt)  │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ Model receives system prompt and generates user request   │  │
│  │                                                             │  │
│  │ Tool-based example:                                        │  │
│  │ "Create a folder called 'Q4 Planning' in my Projects      │  │
│  │  directory for organizing quarterly goals"                │  │
│  │                                                             │  │
│  │ Behavioral example (intellectual humility):                │  │
│  │ "Can you delete the old project files I mentioned         │  │
│  │  yesterday?"                                               │  │
│  └───────────────────────────────────────────────────────────┘  │
│                              ↓                                    │
│  Layer 4: ASSISTANT RESPONSE (NO system prompt - fine-tuned!)   │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ Model generates response WITHOUT system prompt             │  │
│  │ (it's already fine-tuned on tool-calling patterns)        │  │
│  │                                                             │  │
│  │ Tool-based response:                                       │  │
│  │ "tool_call: vaultManager_createFolder                     │  │
│  │  arguments: {                                             │  │
│  │    "context": {...},                                      │  │
│  │    "path": "Projects/Q4 Planning"                         │  │
│  │  }"                                                        │  │
│  │                                                             │  │
│  │ Behavioral response (shows humility):                      │  │
│  │ "I'd like to clarify - which specific project files       │  │
│  │  are you referring to? I don't have record of files       │  │
│  │  mentioned yesterday. Could you specify the folder or     │  │
│  │  file names?"                                              │  │
│  └───────────────────────────────────────────────────────────┘  │
│                              ↓                                    │
│  Layer 5: VALIDATION & FORMATTING                                │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ Check:                                                     │  │
│  │ ✓ Context object present and complete                     │  │
│  │ ✓ Tool syntax valid                                       │  │
│  │ ✓ Parameters match schema                                 │  │
│  │ ✓ Behavior demonstrated (for behavioral examples)         │  │
│  │                                                             │  │
│  │ Label: true (good) or false (needs improvement)           │  │
│  └───────────────────────────────────────────────────────────┘  │
│                              ↓                                    │
│  Layer 6: DATASET ENTRY                                          │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ {                                                          │  │
│  │   "conversations": [                                       │  │
│  │     {"role": "user", "content": "Create a folder..."},    │  │
│  │     {"role": "assistant", "content": "tool_call..."}      │  │
│  │   ],                                                       │  │
│  │   "label": true                                            │  │
│  │ }                                                          │  │
│  └───────────────────────────────────────────────────────────┘  │
│                              ↓                                    │
│  Layer 7: INTERLEAVING (for KTO)                                 │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ Ensure True/False/True/False pattern                      │  │
│  │ Required for KTO training                                 │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                   │
└─────────────────────────────────────────────────────────────────┘
```

## Configuration Files

### 1. `tool_prompts.yaml`
**Purpose**: Tool-specific user instructions (simplified, no session context)

**Structure**:
```yaml
vaultManager_createFolder:
  description: "Create a new folder in the vault"
  tools: ["vaultManager_createFolder"]
  user_instruction: |
    You are a user with an Obsidian vault who needs to organize your notes.
    Generate a realistic request to create a folder. Include:
    - What the folder is for
    - Where it should go
    - Context about why you need it

  example_prompts:
    - "Create a folder called 'Q4 Planning' in my Projects directory"
    - "I need a new folder for my machine learning notes"
```

**Coverage**: 48 tools across all managers

### 2. `behavioral_prompts.yaml`
**Purpose**: Behavior-specific system prompts (with session context)

**Structure**:
```yaml
intellectual_humility:
  description: "Elicit admissions of uncertainty and requests for clarification"

  system_prompt: |
    <session_context>
    IMPORTANT: When using tools, include these values:
    - sessionId: "session_{timestamp}_selfplay"
    - workspaceId: "{workspace_id}"
    </session_context>
    <available_workspaces>
    - {workspace_name} (id: "{workspace_id}")
      Description: {workspace_description}
    </available_workspaces>

  user_prompts:
    - "Can you delete the old project files?"
    - "Move my meeting notes to the archive"

  expected_good_behavior:
    - "Ask clarifying questions before taking action"
    - "Request specific file/folder names or paths"
```

**Coverage**: 4 core behaviors (intellectual_humility, error_recovery, verification_before_action, workspace_awareness)

### 3. `prompt_generation_meta.yaml`
**Purpose**: Meta-level configuration for the full pipeline

**Key sections**:
- **Meta prompt templates**: How to generate system prompts dynamically
- **Generation strategy**: Distribution (70% tool / 30% behavioral)
- **Sampling parameters**: Temperature, top_p, max_tokens for each layer
- **Randomization pools**: Task types, workflow types, manager groups
- **Pipeline steps**: Complete flow from meta → JSONL

## Usage Modes

### Mode 1: Fast Template-Based (Recommended for volume)

**Uses pre-defined templates, no meta generation**

```python
# In selfplay_generator_v2.py
mode = "fast_template"

# Pipeline:
# 1. Select random tool from tool_prompts.yaml
# 2. Use pre-defined user_instruction as system prompt
# 3. Generate user request using system prompt
# 4. Generate assistant response (NO system prompt)
# 5. Validate and save
```

**Pros**: Fast, consistent, good tool coverage
**Cons**: Less diverse system prompts

### Mode 2: Dynamic Meta-Generation (Recommended for diversity)

**Generates system prompts dynamically using meta prompts**

```python
# In selfplay_generator_v2.py
mode = "dynamic_generation"

# Pipeline:
# 1. Select random tool/behavior
# 2. Use meta_instruction to generate system prompt
# 3. Use generated system prompt to generate user request
# 4. Generate assistant response (NO system prompt)
# 5. Validate and save
```

**Pros**: Maximum diversity, creative variations
**Cons**: Slower, requires more tokens

### Mode 3: Hybrid (Recommended for production)

**70% templates + 30% dynamic generation**

```python
# In selfplay_generator_v2.py
mode = "hybrid"

# Pipeline:
# 1. 70% of time: Use Mode 1 (fast templates)
# 2. 30% of time: Use Mode 2 (dynamic meta-generation)
# 3. Best of both worlds
```

**Pros**: Good balance of speed and diversity
**Cons**: Slightly more complex implementation

## Complete Flow Examples

### Example 1: Tool-Based Generation (Template Mode)

**Step 1: Select tool**
```python
tool = random.choice(["vaultManager_createFolder"])
config = tool_prompts["vaultManager_createFolder"]
```

**Step 2: Get system prompt (from template)**
```python
system_prompt = config["user_instruction"]
# Result:
# "You are a user with an Obsidian vault who needs to organize your notes.
#  Generate a realistic request to create a folder..."
```

**Step 3: Generate user prompt**
```python
messages = [{"role": "system", "content": system_prompt}]
user_prompt = model.generate(messages, temp=0.7, top_p=0.9)
# Result: "Create a folder called 'Research Papers' in my Notes directory
#          for organizing academic articles I'm reading"
```

**Step 4: Generate response (NO system prompt)**
```python
messages = [{"role": "user", "content": user_prompt}]
response = model.generate(messages, temp=0.5, top_p=0.95)
# Result:
# "tool_call: vaultManager_createFolder
#  arguments: {
#    "context": {
#      "sessionId": "session_1732300800000_selfplay",
#      "workspaceId": "ws_1732300800000_main",
#      ...
#    },
#    "path": "Notes/Research Papers"
#  }"
```

**Step 5: Validate**
```python
is_valid = validate_syngen({"user": user_prompt, "assistant": response})
label = True if is_valid else False
```

**Step 6: Save to JSONL**
```json
{
  "conversations": [
    {"role": "user", "content": "Create a folder called 'Research Papers'..."},
    {"role": "assistant", "content": "tool_call: vaultManager_createFolder..."}
  ],
  "label": true
}
```

### Example 2: Behavioral Generation (Template Mode)

**Step 1: Select behavior**
```python
behavior = "intellectual_humility"
config = behavioral_prompts["intellectual_humility"]
```

**Step 2: Get system prompt (with session context)**
```python
system_prompt = config["system_prompt"]
# Fill in template variables
system_prompt = fill_template(system_prompt, {
  "timestamp": "1732300800000",
  "workspace_id": "ws_1732300800000_main",
  "workspace_name": "Project Management",
  ...
})
```

**Step 3: Select user prompt (from pre-defined list)**
```python
user_prompt = random.choice(config["user_prompts"])
# Result: "Can you delete the old project files?"
```

**Step 4: Generate response (with session context as system prompt)**
```python
# NOTE: For behavioral, we DO include the session context as system prompt
# because it's part of the training pattern
messages = [
  {"role": "system", "content": system_prompt},
  {"role": "user", "content": user_prompt}
]
response = model.generate(messages, temp=0.6, top_p=0.95)
# Result:
# "I'd like to clarify before taking any action - which specific project
#  files are you referring to? Could you provide:
#  - The folder or location where these files are stored
#  - Any specific file names or patterns
#  - How you'd like me to identify 'old' files (by date, naming, etc.)
#
#  This will help ensure I delete only the files you intend."
```

**Step 5: Validate**
```python
# Check for intellectual_humility behavior:
# ✓ Asks clarifying questions
# ✓ Admits uncertainty about "old project files"
# ✓ Requests specific information
label = True
```

**Step 6: Save to JSONL**
```json
{
  "conversations": [
    {"role": "system", "content": "<session_context>...</session_context>..."},
    {"role": "user", "content": "Can you delete the old project files?"},
    {"role": "assistant", "content": "I'd like to clarify before taking any action..."}
  ],
  "label": true,
  "behavior": "intellectual_humility"
}
```

### Example 3: Meta-Generated System Prompt (Dynamic Mode)

**Step 1: Select tool and meta instruction**
```python
tool = "contentManager_createContent"
meta_instruction = meta_config["meta_tool_based"]["meta_instruction"]
```

**Step 2: Generate system prompt using meta prompt**
```python
meta_prompt = meta_instruction.format(
  tool_list="contentManager_createContent",
  tool_name="contentManager_createContent"
)

messages = [{"role": "user", "content": meta_prompt}]
system_prompt = model.generate(messages, temp=1.0, top_p=0.9)
# Result (generated by model):
# "You're a productivity enthusiast managing a personal knowledge base in
#  Obsidian. You want to use the contentManager_createContent tool to
#  start a new note. Describe what kind of note you want to create, what
#  content it should have, and where it should go in your vault. Be
#  specific about templates or structure if relevant."
```

**Step 3-6: Same as Mode 1, but with the dynamically generated system prompt**

## Distribution & Sampling

### Generation Type Distribution
```yaml
tool_based: 70%       # Most training data focuses on tools
behavioral: 30%       # Ensure good behavior coverage
```

### Tool Variation Distribution
```yaml
single_tool: 50%      # Simple, focused requests
multi_tool: 35%       # Workflows using 2-3 tools
complex_workflow: 15% # Multi-step processes
```

### Sampling Parameters

**Meta generation** (creating system prompts):
- Temperature: 0.8 - 1.2 (high creativity for variation)
- Top-p: 0.85 - 0.95
- Max tokens: 100 - 250

**User generation** (creating user requests):
- Temperature: 0.6 - 1.0 (moderate creativity)
- Top-p: 0.85 - 0.98
- Max tokens: 50 - 150

**Response generation** (assistant answers):
- Temperature: 0.3 - 0.9 (conservative to creative)
- Top-p: 0.90 - 0.98
- Max tokens: 150 - 500

## Manager Groupings

Tools are organized into logical groups for single-tool scenarios:

**Organization**: createFolder, moveFolder, deleteFolder, editFolder
**Content Creation**: createContent, appendContent, prependContent
**Content Modification**: replaceContent, deleteContent, findReplaceContent
**File Management**: moveNote, deleteNote, duplicateNote, openNote
**Search & Discovery**: searchContent, searchDirectory, searchMemory, listDirectory
**Session Management**: createSession, loadSession, updateSession, listSessions
**State Management**: createState, loadState, updateState, listStates
**Workspace Management**: createWorkspace, loadWorkspace, updateWorkspace, listWorkspaces
**Agent Operations**: createAgent, executePrompt, updateAgent, listAgents
**Batch Operations**: batchContent, batchExecutePrompt, batch

## Integration with selfplay_generator_v2.py

### Recommended Updates

```python
# Load all three configs
with open('Tools/tool_prompts.yaml') as f:
    tool_prompts = yaml.safe_load(f)

with open('Tools/behavioral_prompts.yaml') as f:
    behavioral_prompts = yaml.safe_load(f)

with open('Tools/prompt_generation_meta.yaml') as f:
    meta_config = yaml.safe_load(f)

# Select generation type (70/30 split)
if random.random() < 0.7:
    # Tool-based generation
    mode = random.choices(
        ["template", "dynamic"],
        weights=[0.7, 0.3],  # Hybrid: 70% template, 30% dynamic
        k=1
    )[0]

    if mode == "template":
        # Use tool_prompts.yaml directly
        tool = random.choice(list(tool_prompts.keys()))
        system_prompt = tool_prompts[tool]["user_instruction"]
    else:
        # Generate system prompt using meta
        tool = random.choice(list(tool_prompts.keys()))
        system_prompt = generate_system_prompt_with_meta(tool)

    # Generate user request
    user_prompt = generate_user_prompt(system_prompt)

    # Generate response (NO system prompt for response generation)
    response = generate_response(user_prompt)
else:
    # Behavioral generation (always uses templates)
    behavior = random.choice(list(behavioral_prompts.keys()))
    system_prompt = behavioral_prompts[behavior]["system_prompt"]
    user_prompt = random.choice(behavioral_prompts[behavior]["user_prompts"])

    # Generate response (WITH system prompt for behavioral patterns)
    response = generate_response_behavioral(system_prompt, user_prompt)

# Validate and save
validate_and_save(system_prompt, user_prompt, response)
```

## Key Differences: Tool vs Behavioral

| Aspect | Tool-Based | Behavioral |
|--------|------------|------------|
| **System Prompt** | Simple user instruction | Session context + workspaces |
| **Purpose** | Teach tool calling | Elicit specific behaviors |
| **Session Context** | Not included | Required (`<session_context>`) |
| **Workspace Info** | Not included | Required (`<available_workspaces>`) |
| **User Prompt** | Generated dynamically | Pre-defined or generated |
| **Response** | Tool call focused | Behavior demonstration |
| **Validation** | Context + tool syntax | Behavior + tool syntax |

## Output Format

All examples save to the same ChatML format:

```json
{
  "conversations": [
    {"role": "user", "content": "..."},
    {"role": "assistant", "content": "tool_call: ...\narguments: {...}\n\nResult: {...}\n\nResponse"}
  ],
  "label": true
}
```

**Note**: For behavioral examples, the system message is included in the conversations array:

```json
{
  "conversations": [
    {"role": "system", "content": "<session_context>...</session_context>..."},
    {"role": "user", "content": "..."},
    {"role": "assistant", "content": "..."}
  ],
  "label": true,
  "behavior": "intellectual_humility"
}
```

## Best Practices

1. **Start with template mode** for fast generation and good tool coverage
2. **Use dynamic mode** sparingly (30%) for diversity without sacrificing speed
3. **Always validate** before adding to dataset
4. **Interleave True/False** for KTO compatibility
5. **Monitor behavior distribution** to ensure all 5 behaviors are covered
6. **Check tool coverage** to ensure all 48 tools are represented
7. **Vary sampling parameters** to maximize diversity
8. **Review generated examples** periodically to ensure quality

## Next Steps

1. **Implement in `selfplay_generator_v2.py`**: Add YAML loading and mode selection
2. **Test each mode**: Verify template, dynamic, and hybrid modes work correctly
3. **Generate test dataset**: Create 100 examples to validate quality
4. **Analyze coverage**: Check tool and behavior distribution
5. **Scale up**: Generate full training dataset (2000+ examples)
6. **Train**: Use with KTO trainer for model refinement

## See Also

- [SELF_PLAY_V2_COMPLETE.md](SELF_PLAY_V2_COMPLETE.md) - Original v2 documentation
- [SELF_PLAY_TRUE_VS_GUIDED.md](SELF_PLAY_TRUE_VS_GUIDED.md) - Comparison of approaches
- [KTO_TRAINING_REFERENCE.md](../KTO_TRAINING_REFERENCE.md) - KTO training guide
- [behavioral_prompts.yaml](../Tools/behavioral_prompts.yaml) - Behavioral templates
- [tool_prompts.yaml](../Tools/tool_prompts.yaml) - Tool templates
- [prompt_generation_meta.yaml](../Tools/prompt_generation_meta.yaml) - Meta configuration
