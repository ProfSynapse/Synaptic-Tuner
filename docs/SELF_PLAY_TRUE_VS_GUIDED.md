# Self-Play: True Self-Play vs. Guided Evaluation

There are two approaches to generating synthetic data with your fine-tuned model.

## Approach 1: Guided Evaluation-Based (v1)

**File:** `Tools/selfplay_generator.py`

### How it works:
- Uses **pre-written prompts** from `Evaluator/prompts/`
- Model only generates **responses**
- System prompts provide context (session IDs, workspaces)

### Pros:
✅ Comprehensive tool coverage (47 prompts, one per tool)
✅ Consistent, well-formed prompts
✅ Good for initial training
✅ Can compare against eval baseline

### Cons:
❌ **Teaching to the test** - uses evaluation prompts
❌ Not true self-play
❌ Limited diversity (only 47 base prompts)
❌ Prompts may not reflect real usage

### When to use:
- Initial model training
- Ensuring tool coverage
- Benchmarking against evaluation set
- When you need predictable, comprehensive coverage

### Example:
```bash
python Tools/selfplay_generator.py \
  --model claudesidian-mcp \
  --prompt-set Evaluator/prompts/tool_prompts.json \
  --output Datasets/syngen_guided.jsonl \
  --num-examples 1000
```

---

## Approach 2: True Self-Play (v2) ⭐ **RECOMMENDED**

**File:** `Tools/selfplay_generator_v2.py`

### How it works:
- Model generates **both prompts AND responses**
- No pre-written prompts (or uses templates for diversity)
- True self-play loop

### Pros:
✅ **True self-play** - model plays both sides
✅ **Doesn't teach to the test**
✅ **Infinite diversity** - generates new prompts
✅ **Real-world scenarios** - more natural requests
✅ Can use teacher model (Claude/GPT) for prompts

### Cons:
❌ May miss some tools (less comprehensive coverage)
❌ Prompts may be lower quality initially
❌ Harder to compare against eval baseline

### When to use:
- **Recommended for most training**
- After initial SFT training
- When you want diverse, realistic data
- To avoid overfitting to eval prompts
- Continuous self-improvement loops

### Example:
```bash
# Model generates its own prompts
python Tools/selfplay_generator_v2.py \
  --model claudesidian-mcp \
  --output Datasets/syngen_selfplay.jsonl \
  --num-examples 1000

# Or use Claude to generate prompts (higher quality)
python Tools/selfplay_generator_v2.py \
  --model claudesidian-mcp \
  --prompt-generator-model claude-3-5-sonnet-20241022 \
  --output Datasets/syngen_selfplay.jsonl \
  --num-examples 1000
```

---

## Comparison Table

| Feature | Guided (v1) | True Self-Play (v2) |
|---------|-------------|---------------------|
| **Prompt source** | Evaluation prompts | Generated by model |
| **Teaches to test** | ❌ Yes | ✅ No |
| **Tool coverage** | ✅ Comprehensive (47 tools) | ⚠️ Variable |
| **Diversity** | ⚠️ Limited (47 base prompts) | ✅ Infinite |
| **Realism** | ⚠️ Eval-style | ✅ Natural usage |
| **Self-play** | ❌ No (only responses) | ✅ Yes (prompts + responses) |
| **Quality** | ✅ High (curated) | ⚠️ Depends on model |
| **Best for** | Initial training, coverage | Refinement, diversity |

---

## Recommended Workflow

### Phase 1: Initial Training (SFT)
Use your existing curated datasets:
```bash
cd Trainers/rtx3090_sft
python train_sft.py --model-size 7b
```

### Phase 2: Coverage Validation (Guided)
Generate data to ensure tool coverage:
```bash
python Tools/selfplay_generator.py \
  --model your-model-v1 \
  --prompt-set Evaluator/prompts/tool_prompts.json \
  --output Datasets/syngen_coverage.jsonl \
  --num-examples 500
```

### Phase 3: True Self-Play (Diverse)
Generate diverse, realistic data:
```bash
python Tools/selfplay_generator_v2.py \
  --model your-model-v1 \
  --output Datasets/syngen_selfplay.jsonl \
  --num-examples 2000
```

### Phase 4: Combine & Train
```bash
# Combine both datasets
cat Datasets/syngen_coverage.jsonl \
    Datasets/syngen_selfplay.jsonl \
    > Datasets/combined_training.jsonl

# Train with KTO
cd Trainers/rtx3090_kto
python train_kto.py \
  --model-size 7b \
  --local-file ../../Datasets/combined_training.jsonl
```

### Phase 5: Iterate
Repeat Phase 3-4 with the refined model for continuous improvement.

---

## Prompt Generation in True Self-Play

### Method 1: Model Generates Its Own Prompts
The model is asked:
```
"Generate a realistic user request for an Obsidian vault assistant.
Make it specific, actionable, and varied in complexity."
```

**Example outputs:**
- "Create a new folder called 'Q4 Planning' in the Projects directory"
- "Search for all notes about 'deployment' from the past month"
- "Create an agent that summarizes my daily standup notes"

### Method 2: Template-Based Generation
Use randomized templates for diversity:
```python
templates = [
    "Create a new note called '{title}' about {topic}",
    "Search for all notes mentioning '{keyword}' from the past {timeframe}",
    "Create a folder structure for {project}",
    # ... 25+ templates
]
```

**Filled example:**
- "Create a new note called 'Ideas.md' about machine learning"
- "Search for all notes mentioning 'bug' from the past week"

### Method 3: Teacher Model (Future)
Use Claude/GPT API to generate high-quality prompts:
```python
# Claude generates diverse, creative prompts
response = anthropic.messages.create(
    model="claude-3-5-sonnet-20241022",
    messages=[{"role": "user", "content": PROMPT_GENERATION_USER}]
)
```

---

## True Self-Play Flow

```
┌──────────────────────────────────────────────┐
│ 1. Prompt Generation                          │
│    Model/Template: "Create a folder called    │
│                     'Meeting Notes'"          │
└────────────┬─────────────────────────────────┘
             ↓
┌──────────────────────────────────────────────┐
│ 2. Send to Model (NO system prompt)          │
│    User: "Create a folder called              │
│           'Meeting Notes'"                    │
└────────────┬─────────────────────────────────┘
             ↓
┌──────────────────────────────────────────────┐
│ 3. Model Responds                             │
│    "tool_call: vaultManager_createFolder      │
│     arguments: {...}                          │
│     Result: {...}"                            │
└────────────┬─────────────────────────────────┘
             ↓
┌──────────────────────────────────────────────┐
│ 4. Validate Response                          │
│    ✓ Context object present?                  │
│    ✓ All 7 fields present?                    │
│    ✓ Tool parameters correct?                 │
│    → Label: true/false                        │
└────────────┬─────────────────────────────────┘
             ↓
┌──────────────────────────────────────────────┐
│ 5. Collect Example                            │
│    {                                          │
│      "conversations": [                       │
│        {"role": "user", "content": "..."},    │
│        {"role": "assistant", "content": "..."} │
│      ],                                       │
│      "label": true/false                      │
│    }                                          │
└────────────┬─────────────────────────────────┘
             ↓
┌──────────────────────────────────────────────┐
│ 6. Repeat 1000x → Interleave → Save          │
└──────────────────────────────────────────────┘
```

---

## Which Should You Use?

### Use **Guided (v1)** if:
- ✅ First time generating self-play data
- ✅ Need guaranteed tool coverage
- ✅ Want to validate against eval prompts
- ✅ Model is still learning basic patterns

### Use **True Self-Play (v2)** if:
- ✅ Model already understands tool calling
- ✅ Want diverse, realistic training data
- ✅ Avoiding "teaching to the test"
- ✅ Building continuous improvement loop
- ✅ **Most real-world training scenarios**

---

## Best Practice

**Start with v1, transition to v2:**

1. **Initial SFT** - Human-curated data
2. **Coverage check** - v1 with eval prompts (500 examples)
3. **Evaluate** - Check model performance
4. **Self-improve** - v2 for diverse data (2000+ examples)
5. **Retrain** - KTO with mixed data
6. **Iterate** - Repeat step 4-5 for continuous improvement

This gives you:
- ✅ Comprehensive coverage (from v1)
- ✅ Realistic diversity (from v2)
- ✅ No overfitting to eval set
- ✅ Continuous improvement capability
